# Fine-Tune-DistilBERT-for-Text-Classification

This project is a deep learning model designed to classify a disease based on a patient's reported symptoms. The model is fine-tuned using the distilBERT architecture from Hugging Face, which has been trained on a large corpus of text data and is capable of producing high-quality language representations.

### About DistilBERT

BERT (Bidirectional Encoder Representations from Transformers) is a large natural language processing (NLP) model that was developed by Google in 2018. It uses a transformer-based deep neural network architecture to learn accurate language representations from textual data. BERT has been shown to be state-of-the-art model for a large range of NLP tasks, including text classification, question answering, and language translation.

DistilBERT is a variant of the BERT architecture developed by Hugging Face that aims to make the model more efficient. It achieves this by using a smaller architecture with fewer parameters, which allows it to be trained more quickly and deployed on devices with limited computational resources. While DistilBERT has fewer parameters, it has been shown to produce representations that are almost as good as those produced by the larger model. DistilBERT has been shown to outperform BERT in terms of accuracy and efficiency.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danplotkin/Fine-Tune-DistilBERT-for-Text-Classification/blob/main/symptom2disease.ipynb)


